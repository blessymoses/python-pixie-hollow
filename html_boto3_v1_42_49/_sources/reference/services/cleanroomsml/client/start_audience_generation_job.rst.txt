:doc:`CleanRoomsML <../../cleanroomsml>` / Client / start_audience_generation_job

*****************************
start_audience_generation_job
*****************************



.. py:method:: CleanRoomsML.Client.start_audience_generation_job(**kwargs)

  

  Information necessary to start the audience generation job.

  

  See also: `AWS API Documentation <https://docs.aws.amazon.com/goto/WebAPI/cleanroomsml-2023-09-06/StartAudienceGenerationJob>`_  


  **Request Syntax**
  ::

    response = client.start_audience_generation_job(
        name='string',
        configuredAudienceModelArn='string',
        seedAudience={
            'dataSource': {
                's3Uri': 'string'
            },
            'roleArn': 'string',
            'sqlParameters': {
                'queryString': 'string',
                'analysisTemplateArn': 'string',
                'parameters': {
                    'string': 'string'
                }
            },
            'sqlComputeConfiguration': {
                'worker': {
                    'type': 'CR.1X'|'CR.4X',
                    'number': 123,
                    'properties': {
                        'spark': {
                            'string': 'string'
                        }
                    }
                }
            }
        },
        includeSeedInOutput=True|False,
        collaborationId='string',
        description='string',
        tags={
            'string': 'string'
        }
    )
    
  :type name: string
  :param name: **[REQUIRED]** 

    The name of the audience generation job.

    

  
  :type configuredAudienceModelArn: string
  :param configuredAudienceModelArn: **[REQUIRED]** 

    The Amazon Resource Name (ARN) of the configured audience model that is used for this audience generation job.

    

  
  :type seedAudience: dict
  :param seedAudience: **[REQUIRED]** 

    The seed audience that is used to generate the audience.

    

  
    - **dataSource** *(dict) --* 

      Defines the Amazon S3 bucket where the seed audience for the generating audience is stored. A valid data source is a JSON line file in the following format:

       

      ``{"user_id": "111111"}``

       

      ``{"user_id": "222222"}``

       

      ``...``

      

    
      - **s3Uri** *(string) --* **[REQUIRED]** 

        The Amazon S3 location URI.

        

      
    
    - **roleArn** *(string) --* **[REQUIRED]** 

      The ARN of the IAM role that can read the Amazon S3 bucket where the seed audience is stored.

      

    
    - **sqlParameters** *(dict) --* 

      The protected SQL query parameters.

      

    
      - **queryString** *(string) --* 

        The query string to be submitted.

        

      
      - **analysisTemplateArn** *(string) --* 

        The Amazon Resource Name (ARN) associated with the analysis template within a collaboration.

        

      
      - **parameters** *(dict) --* 

        The protected query SQL parameters.

        

      
        - *(string) --* 

        
          - *(string) --* 

          
    
  
    
    - **sqlComputeConfiguration** *(dict) --* 

      Provides configuration information for the instances that will perform the compute work.

      .. note::    This is a Tagged Union structure. Only one of the     following top level keys can be set: ``worker``. 

    
      - **worker** *(dict) --* 

        The worker instances that will perform the compute work.

        

      
        - **type** *(string) --* 

          The instance type of the compute workers that are used.

          

        
        - **number** *(integer) --* 

          The number of compute workers that are used.

          

        
        - **properties** *(dict) --* 

          The configuration properties for the worker compute environment. These properties allow you to customize the compute settings for your Clean Rooms workloads.

          .. note::    This is a Tagged Union structure. Only one of the     following top level keys can be set: ``spark``. 

        
          - **spark** *(dict) --* 

            The Spark configuration properties for SQL workloads. This map contains key-value pairs that configure Apache Spark settings to optimize performance for your data processing jobs. You can specify up to 50 Spark properties, with each key being 1-200 characters and each value being 0-500 characters. These properties allow you to adjust compute capacity for large datasets and complex workloads.

            

          
            - *(string) --* 

            
              - *(string) --* 

              
        
      
        
      
    
  
  :type includeSeedInOutput: boolean
  :param includeSeedInOutput: 

    Whether the seed audience is included in the audience generation output.

    

  
  :type collaborationId: string
  :param collaborationId: 

    The identifier of the collaboration that contains the audience generation job.

    

  
  :type description: string
  :param description: 

    The description of the audience generation job.

    

  
  :type tags: dict
  :param tags: 

    The optional metadata that you apply to the resource to help you categorize and organize them. Each tag consists of a key and an optional value, both of which you define.

     

    The following basic restrictions apply to tags:

     

    
    * Maximum number of tags per resource - 50.
     
    * For each resource, each tag key must be unique, and each tag key can have only one value.
     
    * Maximum key length - 128 Unicode characters in UTF-8.
     
    * Maximum value length - 256 Unicode characters in UTF-8.
     
    * If your tagging schema is used across multiple services and resources, remember that other services may have restrictions on allowed characters. Generally allowed characters are: letters, numbers, and spaces representable in UTF-8, and the following characters: + - = . _ : / @.
     
    * Tag keys and values are case sensitive.
     
    * Do not use aws:, AWS:, or any upper or lowercase combination of such as a prefix for keys as it is reserved for AWS use. You cannot edit or delete tag keys with this prefix. Values can have this prefix. If a tag value has aws as its prefix but the key does not, then Clean Rooms ML considers it to be a user tag and will count against the limit of 50 tags. Tags with only the key prefix of aws do not count against your tags per resource limit.
    

    

  
    - *(string) --* 

    
      - *(string) --* 

      


  
  :rtype: dict
  :returns: 
    
    **Response Syntax**

    
    ::

      {
          'audienceGenerationJobArn': 'string'
      }
      
    **Response Structure**

    

    - *(dict) --* 
      

      - **audienceGenerationJobArn** *(string) --* 

        The Amazon Resource Name (ARN) of the audience generation job.

        
  
  **Exceptions**
  
  *   :py:class:`CleanRoomsML.Client.exceptions.ConflictException`

  
  *   :py:class:`CleanRoomsML.Client.exceptions.ValidationException`

  
  *   :py:class:`CleanRoomsML.Client.exceptions.AccessDeniedException`

  
  *   :py:class:`CleanRoomsML.Client.exceptions.ResourceNotFoundException`

  
  *   :py:class:`CleanRoomsML.Client.exceptions.ThrottlingException`

  
  *   :py:class:`CleanRoomsML.Client.exceptions.ServiceQuotaExceededException`

  