:doc:`MachineLearning <../../machinelearning>` / Client / create_evaluation

*****************
create_evaluation
*****************



.. py:method:: MachineLearning.Client.create_evaluation(**kwargs)

  

  Creates a new ``Evaluation`` of an ``MLModel``. An ``MLModel`` is evaluated on a set of observations associated to a ``DataSource``. Like a ``DataSource`` for an ``MLModel``, the ``DataSource`` for an ``Evaluation`` contains values for the ``Target Variable``. The ``Evaluation`` compares the predicted result for each observation to the actual outcome and provides a summary so that you know how effective the ``MLModel`` functions on the test data. Evaluation generates a relevant performance metric, such as BinaryAUC, RegressionRMSE or MulticlassAvgFScore based on the corresponding ``MLModelType``: ``BINARY``, ``REGRESSION`` or ``MULTICLASS``.

   

  ``CreateEvaluation`` is an asynchronous operation. In response to ``CreateEvaluation``, Amazon Machine Learning (Amazon ML) immediately returns and sets the evaluation status to ``PENDING``. After the ``Evaluation`` is created and ready for use, Amazon ML sets the status to ``COMPLETED``.

   

  You can use the ``GetEvaluation`` operation to check progress of the evaluation during the creation operation.

  

  See also: `AWS API Documentation <https://docs.aws.amazon.com/goto/WebAPI/machinelearning-2014-12-12/CreateEvaluation>`_  


  **Request Syntax**
  ::

    response = client.create_evaluation(
        EvaluationId='string',
        EvaluationName='string',
        MLModelId='string',
        EvaluationDataSourceId='string'
    )
    
  :type EvaluationId: string
  :param EvaluationId: **[REQUIRED]** 

    A user-supplied ID that uniquely identifies the ``Evaluation``.

    

  
  :type EvaluationName: string
  :param EvaluationName: 

    A user-supplied name or description of the ``Evaluation``.

    

  
  :type MLModelId: string
  :param MLModelId: **[REQUIRED]** 

    The ID of the ``MLModel`` to evaluate.

     

    The schema used in creating the ``MLModel`` must match the schema of the ``DataSource`` used in the ``Evaluation``.

    

  
  :type EvaluationDataSourceId: string
  :param EvaluationDataSourceId: **[REQUIRED]** 

    The ID of the ``DataSource`` for the evaluation. The schema of the ``DataSource`` must match the schema used to create the ``MLModel``.

    

  
  
  :rtype: dict
  :returns: 
    
    **Response Syntax**

    
    ::

      {
          'EvaluationId': 'string'
      }
      
    **Response Structure**

    

    - *(dict) --* 

      Represents the output of a ``CreateEvaluation`` operation, and is an acknowledgement that Amazon ML received the request.

       

      ``CreateEvaluation`` operation is asynchronous. You can poll for status updates by using the ``GetEvcaluation`` operation and checking the ``Status`` parameter.

      
      

      - **EvaluationId** *(string) --* 

        The user-supplied ID that uniquely identifies the ``Evaluation``. This value should be identical to the value of the ``EvaluationId`` in the request.

        
  
  **Exceptions**
  
  *   :py:class:`MachineLearning.Client.exceptions.InvalidInputException`

  
  *   :py:class:`MachineLearning.Client.exceptions.InternalServerException`

  
  *   :py:class:`MachineLearning.Client.exceptions.IdempotentParameterMismatchException`

  