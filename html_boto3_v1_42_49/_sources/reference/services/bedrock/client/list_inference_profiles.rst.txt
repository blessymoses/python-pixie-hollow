:doc:`Bedrock <../../bedrock>` / Client / list_inference_profiles

***********************
list_inference_profiles
***********************



.. py:method:: Bedrock.Client.list_inference_profiles(**kwargs)

  

  Returns a list of inference profiles that you can use. For more information, see `Increase throughput and resilience with cross-region inference in Amazon Bedrock <https://docs.aws.amazon.com/bedrock/latest/userguide/cross-region-inference.html>`__. in the Amazon Bedrock User Guide.

  

  See also: `AWS API Documentation <https://docs.aws.amazon.com/goto/WebAPI/bedrock-2023-04-20/ListInferenceProfiles>`_  


  **Request Syntax**
  ::

    response = client.list_inference_profiles(
        maxResults=123,
        nextToken='string',
        typeEquals='SYSTEM_DEFINED'|'APPLICATION'
    )
    
  :type maxResults: integer
  :param maxResults: 

    The maximum number of results to return in the response. If the total number of results is greater than this value, use the token returned in the response in the ``nextToken`` field when making another request to return the next batch of results.

    

  
  :type nextToken: string
  :param nextToken: 

    If the total number of results is greater than the ``maxResults`` value provided in the request, enter the token returned in the ``nextToken`` field in the response in this field to return the next batch of results.

    

  
  :type typeEquals: string
  :param typeEquals: 

    Filters for inference profiles that match the type you specify.

     

    
    * ``SYSTEM_DEFINED`` – The inference profile is defined by Amazon Bedrock. You can route inference requests across regions with these inference profiles.
     
    * ``APPLICATION`` – The inference profile was created by a user. This type of inference profile can track metrics and costs when invoking the model in it. The inference profile may route requests to one or multiple regions.
    

    

  
  
  :rtype: dict
  :returns: 
    
    **Response Syntax**

    
    ::

      {
          'inferenceProfileSummaries': [
              {
                  'inferenceProfileName': 'string',
                  'description': 'string',
                  'createdAt': datetime(2015, 1, 1),
                  'updatedAt': datetime(2015, 1, 1),
                  'inferenceProfileArn': 'string',
                  'models': [
                      {
                          'modelArn': 'string'
                      },
                  ],
                  'inferenceProfileId': 'string',
                  'status': 'ACTIVE',
                  'type': 'SYSTEM_DEFINED'|'APPLICATION'
              },
          ],
          'nextToken': 'string'
      }
      
    **Response Structure**

    

    - *(dict) --* 
      

      - **inferenceProfileSummaries** *(list) --* 

        A list of information about each inference profile that you can use.

        
        

        - *(dict) --* 

          Contains information about an inference profile.

          
          

          - **inferenceProfileName** *(string) --* 

            The name of the inference profile.

            
          

          - **description** *(string) --* 

            The description of the inference profile.

            
          

          - **createdAt** *(datetime) --* 

            The time at which the inference profile was created.

            
          

          - **updatedAt** *(datetime) --* 

            The time at which the inference profile was last updated.

            
          

          - **inferenceProfileArn** *(string) --* 

            The Amazon Resource Name (ARN) of the inference profile.

            
          

          - **models** *(list) --* 

            A list of information about each model in the inference profile.

            
            

            - *(dict) --* 

              Contains information about a model.

              
              

              - **modelArn** *(string) --* 

                The Amazon Resource Name (ARN) of the model.

                
          
        
          

          - **inferenceProfileId** *(string) --* 

            The unique identifier of the inference profile.

            
          

          - **status** *(string) --* 

            The status of the inference profile. ``ACTIVE`` means that the inference profile is ready to be used.

            
          

          - **type** *(string) --* 

            The type of the inference profile. The following types are possible:

             

            
            * ``SYSTEM_DEFINED`` – The inference profile is defined by Amazon Bedrock. You can route inference requests across regions with these inference profiles.
             
            * ``APPLICATION`` – The inference profile was created by a user. This type of inference profile can track metrics and costs when invoking the model in it. The inference profile may route requests to one or multiple regions.
            

            
      
    
      

      - **nextToken** *(string) --* 

        If the total number of results is greater than the ``maxResults`` value provided in the request, use this token when making another request in the ``nextToken`` field to return the next batch of results.

        
  
  **Exceptions**
  
  *   :py:class:`Bedrock.Client.exceptions.AccessDeniedException`

  
  *   :py:class:`Bedrock.Client.exceptions.ValidationException`

  
  *   :py:class:`Bedrock.Client.exceptions.InternalServerException`

  
  *   :py:class:`Bedrock.Client.exceptions.ThrottlingException`

  