:doc:`SageMakerRuntime <../../sagemaker-runtime>` / Client / invoke_endpoint_with_response_stream

************************************
invoke_endpoint_with_response_stream
************************************



.. py:method:: SageMakerRuntime.Client.invoke_endpoint_with_response_stream(**kwargs)

  

  Invokes a model at the specified endpoint to return the inference response as a stream. The inference stream provides the response payload incrementally as a series of parts. Before you can get an inference stream, you must have access to a model that's deployed using Amazon SageMaker AI hosting services, and the container for that model must support inference streaming.

   

  For more information that can help you use this API, see the following sections in the *Amazon SageMaker AI Developer Guide*:

   

  
  * For information about how to add streaming support to a model, see `How Containers Serve Requests <https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-how-containe-serves-requests>`__.
   
  * For information about how to process the streaming response, see `Invoke real-time endpoints <https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-test-endpoints.html>`__.
  

   

  Before you can use this operation, your IAM permissions must allow the ``sagemaker:InvokeEndpoint`` action. For more information about Amazon SageMaker AI actions for IAM policies, see `Actions, resources, and condition keys for Amazon SageMaker AI <https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazonsagemaker.html>`__ in the *IAM Service Authorization Reference*.

   

  Amazon SageMaker AI strips all POST headers except those supported by the API. Amazon SageMaker AI might add additional headers. You should not rely on the behavior of headers outside those enumerated in the request syntax.

   

  Calls to ``InvokeEndpointWithResponseStream`` are authenticated by using Amazon Web Services Signature Version 4. For information, see `Authenticating Requests (Amazon Web Services Signature Version 4) <https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html>`__ in the *Amazon S3 API Reference*.

  

  See also: `AWS API Documentation <https://docs.aws.amazon.com/goto/WebAPI/runtime.sagemaker-2017-05-13/InvokeEndpointWithResponseStream>`_  


  **Request Syntax**
  ::

    response = client.invoke_endpoint_with_response_stream(
        EndpointName='string',
        Body=b'bytes'|file,
        ContentType='string',
        Accept='string',
        CustomAttributes='string',
        TargetVariant='string',
        TargetContainerHostname='string',
        InferenceId='string',
        InferenceComponentName='string',
        SessionId='string'
    )
    
  :type EndpointName: string
  :param EndpointName: **[REQUIRED]** 

    The name of the endpoint that you specified when you created the endpoint using the `CreateEndpoint <https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpoint.html>`__ API.

    

  
  :type Body: bytes or seekable file-like object
  :param Body: **[REQUIRED]** 

    Provides input data, in the format specified in the ``ContentType`` request header. Amazon SageMaker AI passes all of the data in the body to the model.

     

    For information about the format of the request body, see `Common Data Formats-Inference <https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html>`__.

    

  
  :type ContentType: string
  :param ContentType: 

    The MIME type of the input data in the request body.

    

  
  :type Accept: string
  :param Accept: 

    The desired MIME type of the inference response from the model container.

    

  
  :type CustomAttributes: string
  :param CustomAttributes: 

    Provides additional information about a request for an inference submitted to a model hosted at an Amazon SageMaker AI endpoint. The information is an opaque value that is forwarded verbatim. You could use this value, for example, to provide an ID that you can use to track a request or to provide other metadata that a service endpoint was programmed to process. The value must consist of no more than 1024 visible US-ASCII characters as specified in `Section 3.3.6. Field Value Components <https://datatracker.ietf.org/doc/html/rfc7230#section-3.2.6>`__ of the Hypertext Transfer Protocol (HTTP/1.1).

     

    The code in your model is responsible for setting or updating any custom attributes in the response. If your code does not set this value in the response, an empty value is returned. For example, if a custom attribute represents the trace ID, your model can prepend the custom attribute with ``Trace ID:`` in your post-processing function.

     

    This feature is currently supported in the Amazon Web Services SDKs but not in the Amazon SageMaker AI Python SDK.

    

  
  :type TargetVariant: string
  :param TargetVariant: 

    Specify the production variant to send the inference request to when invoking an endpoint that is running two or more variants. Note that this parameter overrides the default behavior for the endpoint, which is to distribute the invocation traffic based on the variant weights.

     

    For information about how to use variant targeting to perform a/b testing, see `Test models in production <https://docs.aws.amazon.com/sagemaker/latest/dg/model-ab-testing.html>`__

    

  
  :type TargetContainerHostname: string
  :param TargetContainerHostname: 

    If the endpoint hosts multiple containers and is configured to use direct invocation, this parameter specifies the host name of the container to invoke.

    

  
  :type InferenceId: string
  :param InferenceId: 

    An identifier that you assign to your request.

    

  
  :type InferenceComponentName: string
  :param InferenceComponentName: 

    If the endpoint hosts one or more inference components, this parameter specifies the name of inference component to invoke for a streaming response.

    

  
  :type SessionId: string
  :param SessionId: 

    The ID of a stateful session to handle your request.

     

    You can't create a stateful session by using the ``InvokeEndpointWithResponseStream`` action. Instead, you can create one by using the ``InvokeEndpoint`` action. In your request, you specify ``NEW_SESSION`` for the ``SessionId`` request parameter. The response to that request provides the session ID for the ``NewSessionId`` response parameter.

    

  
  
  :rtype: dict
  :returns: 
    

    The response of this operation contains an :class:`.EventStream` member. When iterated the :class:`.EventStream` will yield events based on the structure below, where only one of the top level keys will be present for any given event.
    
    **Response Syntax**

    
    ::

      {
          'Body': EventStream({
              'PayloadPart': {
                  'Bytes': b'bytes'
              },
              'ModelStreamError': {
                  'Message': 'string',
                  'ErrorCode': 'string'
              },
              'InternalStreamFailure': {
                  'Message': 'string'
              }
          }),
          'ContentType': 'string',
          'InvokedProductionVariant': 'string',
          'CustomAttributes': 'string'
      }
      
    **Response Structure**

    

    - *(dict) --* 
      

      - **Body** (:class:`.EventStream`) -- 

        A stream of payload parts. Each part contains a portion of the response for a streaming inference request.

        
        

        - **PayloadPart** *(dict) --* 

          A wrapper for pieces of the payload that's returned in response to a streaming inference request. A streaming inference response consists of one or more payload parts.

          
          

          - **Bytes** *(bytes) --* 

            A blob that contains part of the response for your streaming inference request.

            
      
        

        - **ModelStreamError** *(dict) --* 

          An error occurred while streaming the response body. This error can have the following error codes:

            ModelInvocationTimeExceeded  

          The model failed to finish sending the response within the timeout period allowed by Amazon SageMaker AI.

            StreamBroken  

          The Transmission Control Protocol (TCP) connection between the client and the model was reset or closed.

          
          

          - **Message** *(string) --* 
          

          - **ErrorCode** *(string) --* 

            This error can have the following error codes:

              ModelInvocationTimeExceeded  

            The model failed to finish sending the response within the timeout period allowed by Amazon SageMaker AI.

              StreamBroken  

            The Transmission Control Protocol (TCP) connection between the client and the model was reset or closed.

            
      
        

        - **InternalStreamFailure** *(dict) --* 

          The stream processing failed because of an unknown error, exception or failure. Try your request again.

          
          

          - **Message** *(string) --* 
      
    
      

      - **ContentType** *(string) --* 

        The MIME type of the inference returned from the model container.

        
      

      - **InvokedProductionVariant** *(string) --* 

        Identifies the production variant that was invoked.

        
      

      - **CustomAttributes** *(string) --* 

        Provides additional information in the response about the inference returned by a model hosted at an Amazon SageMaker AI endpoint. The information is an opaque value that is forwarded verbatim. You could use this value, for example, to return an ID received in the ``CustomAttributes`` header of a request or other metadata that a service endpoint was programmed to produce. The value must consist of no more than 1024 visible US-ASCII characters as specified in `Section 3.3.6. Field Value Components <https://tools.ietf.org/html/rfc7230#section-3.2.6>`__ of the Hypertext Transfer Protocol (HTTP/1.1). If the customer wants the custom attribute returned, the model must set the custom attribute to be included on the way back.

         

        The code in your model is responsible for setting or updating any custom attributes in the response. If your code does not set this value in the response, an empty value is returned. For example, if a custom attribute represents the trace ID, your model can prepend the custom attribute with ``Trace ID:`` in your post-processing function.

         

        This feature is currently supported in the Amazon Web Services SDKs but not in the Amazon SageMaker AI Python SDK.

        
  
  **Exceptions**
  
  *   :py:class:`SageMakerRuntime.Client.exceptions.InternalFailure`

  
  *   :py:class:`SageMakerRuntime.Client.exceptions.ServiceUnavailable`

  
  *   :py:class:`SageMakerRuntime.Client.exceptions.ValidationError`

  
  *   :py:class:`SageMakerRuntime.Client.exceptions.ModelError`

  
  *   :py:class:`SageMakerRuntime.Client.exceptions.ModelStreamError`

  
  *   :py:class:`SageMakerRuntime.Client.exceptions.InternalStreamFailure`

  