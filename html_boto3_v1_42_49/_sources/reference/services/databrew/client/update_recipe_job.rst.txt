:doc:`GlueDataBrew <../../databrew>` / Client / update_recipe_job

*****************
update_recipe_job
*****************



.. py:method:: GlueDataBrew.Client.update_recipe_job(**kwargs)

  

  Modifies the definition of an existing DataBrew recipe job.

  

  See also: `AWS API Documentation <https://docs.aws.amazon.com/goto/WebAPI/databrew-2017-07-25/UpdateRecipeJob>`_  


  **Request Syntax**
  ::

    response = client.update_recipe_job(
        EncryptionKeyArn='string',
        EncryptionMode='SSE-KMS'|'SSE-S3',
        Name='string',
        LogSubscription='ENABLE'|'DISABLE',
        MaxCapacity=123,
        MaxRetries=123,
        Outputs=[
            {
                'CompressionFormat': 'GZIP'|'LZ4'|'SNAPPY'|'BZIP2'|'DEFLATE'|'LZO'|'BROTLI'|'ZSTD'|'ZLIB',
                'Format': 'CSV'|'JSON'|'PARQUET'|'GLUEPARQUET'|'AVRO'|'ORC'|'XML'|'TABLEAUHYPER',
                'PartitionColumns': [
                    'string',
                ],
                'Location': {
                    'Bucket': 'string',
                    'Key': 'string',
                    'BucketOwner': 'string'
                },
                'Overwrite': True|False,
                'FormatOptions': {
                    'Csv': {
                        'Delimiter': 'string'
                    }
                },
                'MaxOutputFiles': 123
            },
        ],
        DataCatalogOutputs=[
            {
                'CatalogId': 'string',
                'DatabaseName': 'string',
                'TableName': 'string',
                'S3Options': {
                    'Location': {
                        'Bucket': 'string',
                        'Key': 'string',
                        'BucketOwner': 'string'
                    }
                },
                'DatabaseOptions': {
                    'TempDirectory': {
                        'Bucket': 'string',
                        'Key': 'string',
                        'BucketOwner': 'string'
                    },
                    'TableName': 'string'
                },
                'Overwrite': True|False
            },
        ],
        DatabaseOutputs=[
            {
                'GlueConnectionName': 'string',
                'DatabaseOptions': {
                    'TempDirectory': {
                        'Bucket': 'string',
                        'Key': 'string',
                        'BucketOwner': 'string'
                    },
                    'TableName': 'string'
                },
                'DatabaseOutputMode': 'NEW_TABLE'
            },
        ],
        RoleArn='string',
        Timeout=123
    )
    
  :type EncryptionKeyArn: string
  :param EncryptionKeyArn: 

    The Amazon Resource Name (ARN) of an encryption key that is used to protect the job.

    

  
  :type EncryptionMode: string
  :param EncryptionMode: 

    The encryption mode for the job, which can be one of the following:

     

    
    * ``SSE-KMS`` - Server-side encryption with keys managed by KMS.
     
    * ``SSE-S3`` - Server-side encryption with keys managed by Amazon S3.
    

    

  
  :type Name: string
  :param Name: **[REQUIRED]** 

    The name of the job to update.

    

  
  :type LogSubscription: string
  :param LogSubscription: 

    Enables or disables Amazon CloudWatch logging for the job. If logging is enabled, CloudWatch writes one log stream for each job run.

    

  
  :type MaxCapacity: integer
  :param MaxCapacity: 

    The maximum number of nodes that DataBrew can consume when the job processes data.

    

  
  :type MaxRetries: integer
  :param MaxRetries: 

    The maximum number of times to retry the job after a job run fails.

    

  
  :type Outputs: list
  :param Outputs: 

    One or more artifacts that represent the output from running the job.

    

  
    - *(dict) --* 

      Represents options that specify how and where in Amazon S3 DataBrew writes the output generated by recipe jobs or profile jobs.

      

    
      - **CompressionFormat** *(string) --* 

        The compression algorithm used to compress the output text of the job.

        

      
      - **Format** *(string) --* 

        The data format of the output of the job.

        

      
      - **PartitionColumns** *(list) --* 

        The names of one or more partition columns for the output of the job.

        

      
        - *(string) --* 

        
    
      - **Location** *(dict) --* **[REQUIRED]** 

        The location in Amazon S3 where the job writes its output.

        

      
        - **Bucket** *(string) --* **[REQUIRED]** 

          The Amazon S3 bucket name.

          

        
        - **Key** *(string) --* 

          The unique name of the object in the bucket.

          

        
        - **BucketOwner** *(string) --* 

          The Amazon Web Services account ID of the bucket owner.

          

        
      
      - **Overwrite** *(boolean) --* 

        A value that, if true, means that any data in the location specified for output is overwritten with new output.

        

      
      - **FormatOptions** *(dict) --* 

        Represents options that define how DataBrew formats job output files.

        

      
        - **Csv** *(dict) --* 

          Represents a set of options that define the structure of comma-separated value (CSV) job output.

          

        
          - **Delimiter** *(string) --* 

            A single character that specifies the delimiter used to create CSV job output.

            

          
        
      
      - **MaxOutputFiles** *(integer) --* 

        Maximum number of files to be generated by the job and written to the output folder. For output partitioned by column(s), the MaxOutputFiles value is the maximum number of files per partition.

        

      
    

  :type DataCatalogOutputs: list
  :param DataCatalogOutputs: 

    One or more artifacts that represent the Glue Data Catalog output from running the job.

    

  
    - *(dict) --* 

      Represents options that specify how and where in the Glue Data Catalog DataBrew writes the output generated by recipe jobs.

      

    
      - **CatalogId** *(string) --* 

        The unique identifier of the Amazon Web Services account that holds the Data Catalog that stores the data.

        

      
      - **DatabaseName** *(string) --* **[REQUIRED]** 

        The name of a database in the Data Catalog.

        

      
      - **TableName** *(string) --* **[REQUIRED]** 

        The name of a table in the Data Catalog.

        

      
      - **S3Options** *(dict) --* 

        Represents options that specify how and where DataBrew writes the Amazon S3 output generated by recipe jobs.

        

      
        - **Location** *(dict) --* **[REQUIRED]** 

          Represents an Amazon S3 location (bucket name and object key) where DataBrew can write output from a job.

          

        
          - **Bucket** *(string) --* **[REQUIRED]** 

            The Amazon S3 bucket name.

            

          
          - **Key** *(string) --* 

            The unique name of the object in the bucket.

            

          
          - **BucketOwner** *(string) --* 

            The Amazon Web Services account ID of the bucket owner.

            

          
        
      
      - **DatabaseOptions** *(dict) --* 

        Represents options that specify how and where DataBrew writes the database output generated by recipe jobs.

        

      
        - **TempDirectory** *(dict) --* 

          Represents an Amazon S3 location (bucket name and object key) where DataBrew can store intermediate results.

          

        
          - **Bucket** *(string) --* **[REQUIRED]** 

            The Amazon S3 bucket name.

            

          
          - **Key** *(string) --* 

            The unique name of the object in the bucket.

            

          
          - **BucketOwner** *(string) --* 

            The Amazon Web Services account ID of the bucket owner.

            

          
        
        - **TableName** *(string) --* **[REQUIRED]** 

          A prefix for the name of a table DataBrew will create in the database.

          

        
      
      - **Overwrite** *(boolean) --* 

        A value that, if true, means that any data in the location specified for output is overwritten with new output. Not supported with DatabaseOptions.

        

      
    

  :type DatabaseOutputs: list
  :param DatabaseOutputs: 

    Represents a list of JDBC database output objects which defines the output destination for a DataBrew recipe job to write into.

    

  
    - *(dict) --* 

      Represents a JDBC database output object which defines the output destination for a DataBrew recipe job to write into.

      

    
      - **GlueConnectionName** *(string) --* **[REQUIRED]** 

        The Glue connection that stores the connection information for the target database.

        

      
      - **DatabaseOptions** *(dict) --* **[REQUIRED]** 

        Represents options that specify how and where DataBrew writes the database output generated by recipe jobs.

        

      
        - **TempDirectory** *(dict) --* 

          Represents an Amazon S3 location (bucket name and object key) where DataBrew can store intermediate results.

          

        
          - **Bucket** *(string) --* **[REQUIRED]** 

            The Amazon S3 bucket name.

            

          
          - **Key** *(string) --* 

            The unique name of the object in the bucket.

            

          
          - **BucketOwner** *(string) --* 

            The Amazon Web Services account ID of the bucket owner.

            

          
        
        - **TableName** *(string) --* **[REQUIRED]** 

          A prefix for the name of a table DataBrew will create in the database.

          

        
      
      - **DatabaseOutputMode** *(string) --* 

        The output mode to write into the database. Currently supported option: NEW_TABLE.

        

      
    

  :type RoleArn: string
  :param RoleArn: **[REQUIRED]** 

    The Amazon Resource Name (ARN) of the Identity and Access Management (IAM) role to be assumed when DataBrew runs the job.

    

  
  :type Timeout: integer
  :param Timeout: 

    The job's timeout in minutes. A job that attempts to run longer than this timeout period ends with a status of ``TIMEOUT``.

    

  
  
  :rtype: dict
  :returns: 
    
    **Response Syntax**

    
    ::

      {
          'Name': 'string'
      }
      
    **Response Structure**

    

    - *(dict) --* 
      

      - **Name** *(string) --* 

        The name of the job that you updated.

        
  
  **Exceptions**
  
  *   :py:class:`GlueDataBrew.Client.exceptions.AccessDeniedException`

  
  *   :py:class:`GlueDataBrew.Client.exceptions.ResourceNotFoundException`

  
  *   :py:class:`GlueDataBrew.Client.exceptions.ValidationException`

  