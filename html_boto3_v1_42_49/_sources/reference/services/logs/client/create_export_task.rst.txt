:doc:`CloudWatchLogs <../../logs>` / Client / create_export_task

******************
create_export_task
******************



.. py:method:: CloudWatchLogs.Client.create_export_task(**kwargs)

  

  Creates an export task so that you can efficiently export data from a log group to an Amazon S3 bucket. When you perform a ``CreateExportTask`` operation, you must use credentials that have permission to write to the S3 bucket that you specify as the destination.

   

  Exporting log data to S3 buckets that are encrypted by KMS is supported. Exporting log data to Amazon S3 buckets that have S3 Object Lock enabled with a retention period is also supported.

   

  Exporting to S3 buckets that are encrypted with AES-256 is supported.

   

  This is an asynchronous call. If all the required information is provided, this operation initiates an export task and responds with the ID of the task. After the task has started, you can use `DescribeExportTasks <https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_DescribeExportTasks.html>`__ to get the status of the export task. Each account can only have one active ( ``RUNNING`` or ``PENDING``) export task at a time. To cancel an export task, use `CancelExportTask <https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_CancelExportTask.html>`__.

   

  You can export logs from multiple log groups or multiple time ranges to the same S3 bucket. To separate log data for each export task, specify a prefix to be used as the Amazon S3 key prefix for all exported objects.

   

  .. note::

    

    We recommend that you don't regularly export to Amazon S3 as a way to continuously archive your logs. For that use case, we instead recommend that you use subscriptions. For more information about subscriptions, see `Real-time processing of log data with subscriptions <https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/Subscriptions.html>`__.

    

   

  .. note::

    

    Time-based sorting on chunks of log data inside an exported file is not guaranteed. You can sort the exported log field data by using Linux utilities.

    

  

  See also: `AWS API Documentation <https://docs.aws.amazon.com/goto/WebAPI/logs-2014-03-28/CreateExportTask>`_  


  **Request Syntax**
  ::

    response = client.create_export_task(
        taskName='string',
        logGroupName='string',
        logStreamNamePrefix='string',
        fromTime=123,
        to=123,
        destination='string',
        destinationPrefix='string'
    )
    
  :type taskName: string
  :param taskName: 

    The name of the export task.

    

  
  :type logGroupName: string
  :param logGroupName: **[REQUIRED]** 

    The name of the log group.

    

  
  :type logStreamNamePrefix: string
  :param logStreamNamePrefix: 

    Export only log streams that match the provided prefix. If you don't specify a value, no prefix filter is applied.

    

  
  :type fromTime: integer
  :param fromTime: **[REQUIRED]** 

    The start time of the range for the request, expressed as the number of milliseconds after ``Jan 1, 1970 00:00:00 UTC``. Events with a timestamp earlier than this time are not exported.

    

  
  :type to: integer
  :param to: **[REQUIRED]** 

    The end time of the range for the request, expressed as the number of milliseconds after ``Jan 1, 1970 00:00:00 UTC``. Events with a timestamp later than this time are not exported.

     

    You must specify a time that is not earlier than when this log group was created.

    

  
  :type destination: string
  :param destination: **[REQUIRED]** 

    The name of S3 bucket for the exported log data. The bucket must be in the same Amazon Web Services Region.

    

  
  :type destinationPrefix: string
  :param destinationPrefix: 

    The prefix used as the start of the key for every object exported. If you don't specify a value, the default is ``exportedlogs``.

     

    The length of this parameter must comply with the S3 object key name length limits. The object key name is a sequence of Unicode characters with UTF-8 encoding, and can be up to 1,024 bytes.

    

  
  
  :rtype: dict
  :returns: 
    
    **Response Syntax**

    
    ::

      {
          'taskId': 'string'
      }
      
    **Response Structure**

    

    - *(dict) --* 
      

      - **taskId** *(string) --* 

        The ID of the export task.

        
  
  **Exceptions**
  
  *   :py:class:`CloudWatchLogs.Client.exceptions.InvalidParameterException`

  
  *   :py:class:`CloudWatchLogs.Client.exceptions.LimitExceededException`

  
  *   :py:class:`CloudWatchLogs.Client.exceptions.OperationAbortedException`

  
  *   :py:class:`CloudWatchLogs.Client.exceptions.ServiceUnavailableException`

  
  *   :py:class:`CloudWatchLogs.Client.exceptions.ResourceNotFoundException`

  
  *   :py:class:`CloudWatchLogs.Client.exceptions.ResourceAlreadyExistsException`

  