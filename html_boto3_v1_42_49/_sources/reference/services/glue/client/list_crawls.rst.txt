:doc:`Glue <../../glue>` / Client / list_crawls

***********
list_crawls
***********



.. py:method:: Glue.Client.list_crawls(**kwargs)

  

  Returns all the crawls of a specified crawler. Returns only the crawls that have occurred since the launch date of the crawler history feature, and only retains up to 12 months of crawls. Older crawls will not be returned.

   

  You may use this API to:

   

  
  * Retrive all the crawls of a specified crawler.
   
  * Retrieve all the crawls of a specified crawler within a limited count.
   
  * Retrieve all the crawls of a specified crawler in a specific time range.
   
  * Retrieve all the crawls of a specified crawler with a particular state, crawl ID, or DPU hour value.
  

  

  See also: `AWS API Documentation <https://docs.aws.amazon.com/goto/WebAPI/glue-2017-03-31/ListCrawls>`_  


  **Request Syntax**
  ::

    response = client.list_crawls(
        CrawlerName='string',
        MaxResults=123,
        Filters=[
            {
                'FieldName': 'CRAWL_ID'|'STATE'|'START_TIME'|'END_TIME'|'DPU_HOUR',
                'FilterOperator': 'GT'|'GE'|'LT'|'LE'|'EQ'|'NE',
                'FieldValue': 'string'
            },
        ],
        NextToken='string'
    )
    
  :type CrawlerName: string
  :param CrawlerName: **[REQUIRED]** 

    The name of the crawler whose runs you want to retrieve.

    

  
  :type MaxResults: integer
  :param MaxResults: 

    The maximum number of results to return. The default is 20, and maximum is 100.

    

  
  :type Filters: list
  :param Filters: 

    Filters the crawls by the criteria you specify in a list of ``CrawlsFilter`` objects.

    

  
    - *(dict) --* 

      A list of fields, comparators and value that you can use to filter the crawler runs for a specified crawler.

      

    
      - **FieldName** *(string) --* 

        A key used to filter the crawler runs for a specified crawler. Valid values for each of the field names are:

         

        
        * ``CRAWL_ID``: A string representing the UUID identifier for a crawl.
         
        * ``STATE``: A string representing the state of the crawl.
         
        * ``START_TIME`` and ``END_TIME``: The epoch timestamp in milliseconds.
         
        * ``DPU_HOUR``: The number of data processing unit (DPU) hours used for the crawl.
        

        

      
      - **FilterOperator** *(string) --* 

        A defined comparator that operates on the value. The available operators are:

         

        
        * ``GT``: Greater than.
         
        * ``GE``: Greater than or equal to.
         
        * ``LT``: Less than.
         
        * ``LE``: Less than or equal to.
         
        * ``EQ``: Equal to.
         
        * ``NE``: Not equal to.
        

        

      
      - **FieldValue** *(string) --* 

        The value provided for comparison on the crawl field.

        

      
    

  :type NextToken: string
  :param NextToken: 

    A continuation token, if this is a continuation call.

    

  
  
  :rtype: dict
  :returns: 
    
    **Response Syntax**

    
    ::

      {
          'Crawls': [
              {
                  'CrawlId': 'string',
                  'State': 'RUNNING'|'COMPLETED'|'FAILED'|'STOPPED',
                  'StartTime': datetime(2015, 1, 1),
                  'EndTime': datetime(2015, 1, 1),
                  'Summary': 'string',
                  'ErrorMessage': 'string',
                  'LogGroup': 'string',
                  'LogStream': 'string',
                  'MessagePrefix': 'string',
                  'DPUHour': 123.0
              },
          ],
          'NextToken': 'string'
      }
      
    **Response Structure**

    

    - *(dict) --* 
      

      - **Crawls** *(list) --* 

        A list of ``CrawlerHistory`` objects representing the crawl runs that meet your criteria.

        
        

        - *(dict) --* 

          Contains the information for a run of a crawler.

          
          

          - **CrawlId** *(string) --* 

            A UUID identifier for each crawl.

            
          

          - **State** *(string) --* 

            The state of the crawl.

            
          

          - **StartTime** *(datetime) --* 

            The date and time on which the crawl started.

            
          

          - **EndTime** *(datetime) --* 

            The date and time on which the crawl ended.

            
          

          - **Summary** *(string) --* 

            A run summary for the specific crawl in JSON. Contains the catalog tables and partitions that were added, updated, or deleted.

            
          

          - **ErrorMessage** *(string) --* 

            If an error occurred, the error message associated with the crawl.

            
          

          - **LogGroup** *(string) --* 

            The log group associated with the crawl.

            
          

          - **LogStream** *(string) --* 

            The log stream associated with the crawl.

            
          

          - **MessagePrefix** *(string) --* 

            The prefix for a CloudWatch message about this crawl.

            
          

          - **DPUHour** *(float) --* 

            The number of data processing units (DPU) used in hours for the crawl.

            
      
    
      

      - **NextToken** *(string) --* 

        A continuation token for paginating the returned list of tokens, returned if the current segment of the list is not the last.

        
  
  **Exceptions**
  
  *   :py:class:`Glue.Client.exceptions.EntityNotFoundException`

  
  *   :py:class:`Glue.Client.exceptions.OperationTimeoutException`

  
  *   :py:class:`Glue.Client.exceptions.InvalidInputException`

  