:doc:`Glue <../../glue>` / Client / start_ml_evaluation_task_run

****************************
start_ml_evaluation_task_run
****************************



.. py:method:: Glue.Client.start_ml_evaluation_task_run(**kwargs)

  

  Starts a task to estimate the quality of the transform.

   

  When you provide label sets as examples of truth, Glue machine learning uses some of those examples to learn from them. The rest of the labels are used as a test to estimate quality.

   

  Returns a unique identifier for the run. You can call ``GetMLTaskRun`` to get more information about the stats of the ``EvaluationTaskRun``.

  

  See also: `AWS API Documentation <https://docs.aws.amazon.com/goto/WebAPI/glue-2017-03-31/StartMLEvaluationTaskRun>`_  


  **Request Syntax**
  ::

    response = client.start_ml_evaluation_task_run(
        TransformId='string'
    )
    
  :type TransformId: string
  :param TransformId: **[REQUIRED]** 

    The unique identifier of the machine learning transform.

    

  
  
  :rtype: dict
  :returns: 
    
    **Response Syntax**

    
    ::

      {
          'TaskRunId': 'string'
      }
      
    **Response Structure**

    

    - *(dict) --* 
      

      - **TaskRunId** *(string) --* 

        The unique identifier associated with this run.

        
  
  **Exceptions**
  
  *   :py:class:`Glue.Client.exceptions.EntityNotFoundException`

  
  *   :py:class:`Glue.Client.exceptions.InvalidInputException`

  
  *   :py:class:`Glue.Client.exceptions.OperationTimeoutException`

  
  *   :py:class:`Glue.Client.exceptions.InternalServiceException`

  
  *   :py:class:`Glue.Client.exceptions.ConcurrentRunsExceededException`

  
  *   :py:class:`Glue.Client.exceptions.MLTransformNotReadyException`

  