:doc:`Comprehend <../../comprehend>` / Client / detect_toxic_content

********************
detect_toxic_content
********************



.. py:method:: Comprehend.Client.detect_toxic_content(**kwargs)

  

  Performs toxicity analysis on the list of text strings that you provide as input. The API response contains a results list that matches the size of the input list. For more information about toxicity detection, see `Toxicity detection <https://docs.aws.amazon.com/comprehend/latest/dg/toxicity-detection.html>`__ in the *Amazon Comprehend Developer Guide*.

  

  See also: `AWS API Documentation <https://docs.aws.amazon.com/goto/WebAPI/comprehend-2017-11-27/DetectToxicContent>`_  


  **Request Syntax**
  ::

    response = client.detect_toxic_content(
        TextSegments=[
            {
                'Text': 'string'
            },
        ],
        LanguageCode='en'|'es'|'fr'|'de'|'it'|'pt'|'ar'|'hi'|'ja'|'ko'|'zh'|'zh-TW'
    )
    
  :type TextSegments: list
  :param TextSegments: **[REQUIRED]** 

    A list of up to 10 text strings. Each string has a maximum size of 1 KB, and the maximum size of the list is 10 KB.

    

  
    - *(dict) --* 

      One of the of text strings. Each string has a size limit of 1KB.

      

    
      - **Text** *(string) --* **[REQUIRED]** 

        The text content.

        

      
    

  :type LanguageCode: string
  :param LanguageCode: **[REQUIRED]** 

    The language of the input text. Currently, English is the only supported language.

    

  
  
  :rtype: dict
  :returns: 
    
    **Response Syntax**

    
    ::

      {
          'ResultList': [
              {
                  'Labels': [
                      {
                          'Name': 'GRAPHIC'|'HARASSMENT_OR_ABUSE'|'HATE_SPEECH'|'INSULT'|'PROFANITY'|'SEXUAL'|'VIOLENCE_OR_THREAT',
                          'Score': ...
                      },
                  ],
                  'Toxicity': ...
              },
          ]
      }
      
    **Response Structure**

    

    - *(dict) --* 
      

      - **ResultList** *(list) --* 

        Results of the content moderation analysis. Each entry in the results list contains a list of toxic content types identified in the text, along with a confidence score for each content type. The results list also includes a toxicity score for each entry in the results list.

        
        

        - *(dict) --* 

          Toxicity analysis result for one string. For more information about toxicity detection, see `Toxicity detection <https://docs.aws.amazon.com/comprehend/latest/dg/toxicity-detection.html>`__ in the *Amazon Comprehend Developer Guide*.

          
          

          - **Labels** *(list) --* 

            Array of toxic content types identified in the string.

            
            

            - *(dict) --* 

              Toxic content analysis result for one string. For more information about toxicity detection, see `Toxicity detection <https://docs.aws.amazon.com/comprehend/latest/dg/toxicity-detection.html>`__ in the *Amazon Comprehend Developer Guide*

              
              

              - **Name** *(string) --* 

                The name of the toxic content type.

                
              

              - **Score** *(float) --* 

                Model confidence in the detected content type. Value range is zero to one, where one is highest confidence.

                
          
        
          

          - **Toxicity** *(float) --* 

            Overall toxicity score for the string. Value range is zero to one, where one is the highest confidence.

            
      
    
  
  **Exceptions**
  
  *   :py:class:`Comprehend.Client.exceptions.InvalidRequestException`

  
  *   :py:class:`Comprehend.Client.exceptions.TextSizeLimitExceededException`

  
  *   :py:class:`Comprehend.Client.exceptions.UnsupportedLanguageException`

  
  *   :py:class:`Comprehend.Client.exceptions.InternalServerException`

  