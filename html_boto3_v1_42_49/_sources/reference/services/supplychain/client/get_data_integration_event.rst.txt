:doc:`SupplyChain <../../supplychain>` / Client / get_data_integration_event

**************************
get_data_integration_event
**************************



.. py:method:: SupplyChain.Client.get_data_integration_event(**kwargs)

  

  Enables you to programmatically view an Amazon Web Services Supply Chain Data Integration Event. Developers can view the eventType, eventGroupId, eventTimestamp, datasetTarget, datasetLoadExecution.

  

  See also: `AWS API Documentation <https://docs.aws.amazon.com/goto/WebAPI/supplychain-2024-01-01/GetDataIntegrationEvent>`_  


  **Request Syntax**
  ::

    response = client.get_data_integration_event(
        instanceId='string',
        eventId='string'
    )
    
  :type instanceId: string
  :param instanceId: **[REQUIRED]** 

    The Amazon Web Services Supply Chain instance identifier.

    

  
  :type eventId: string
  :param eventId: **[REQUIRED]** 

    The unique event identifier.

    

  
  
  :rtype: dict
  :returns: 
    
    **Response Syntax**

    
    ::

      {
          'event': {
              'instanceId': 'string',
              'eventId': 'string',
              'eventType': 'scn.data.forecast'|'scn.data.inventorylevel'|'scn.data.inboundorder'|'scn.data.inboundorderline'|'scn.data.inboundorderlineschedule'|'scn.data.outboundorderline'|'scn.data.outboundshipment'|'scn.data.processheader'|'scn.data.processoperation'|'scn.data.processproduct'|'scn.data.reservation'|'scn.data.shipment'|'scn.data.shipmentstop'|'scn.data.shipmentstoporder'|'scn.data.supplyplan'|'scn.data.dataset',
              'eventGroupId': 'string',
              'eventTimestamp': datetime(2015, 1, 1),
              'datasetTargetDetails': {
                  'datasetIdentifier': 'string',
                  'operationType': 'APPEND'|'UPSERT'|'DELETE',
                  'datasetLoadExecution': {
                      'status': 'SUCCEEDED'|'IN_PROGRESS'|'FAILED',
                      'message': 'string'
                  }
              }
          }
      }
      
    **Response Structure**

    

    - *(dict) --* 

      The response parameters for GetDataIntegrationEvent.

      
      

      - **event** *(dict) --* 

        The details of the DataIntegrationEvent returned.

        
        

        - **instanceId** *(string) --* 

          The AWS Supply Chain instance identifier.

          
        

        - **eventId** *(string) --* 

          The unique event identifier.

          
        

        - **eventType** *(string) --* 

          The data event type.

          
        

        - **eventGroupId** *(string) --* 

          Event identifier (for example, orderId for InboundOrder) used for data sharding or partitioning.

          
        

        - **eventTimestamp** *(datetime) --* 

          The event timestamp (in epoch seconds).

          
        

        - **datasetTargetDetails** *(dict) --* 

          The target dataset details for a DATASET event type.

          
          

          - **datasetIdentifier** *(string) --* 

            The datalake dataset ARN identifier.

            
          

          - **operationType** *(string) --* 

            The target dataset load operation type. The available options are:

             

            
            * **APPEND** - Add new records to the dataset. Noted that this operation type will just try to append records as-is without any primary key or partition constraints.
             
            * **UPSERT** - Modify existing records in the dataset with primary key configured, events for datasets without primary keys are not allowed. If event data contains primary keys that match records in the dataset within same partition, then those existing records (in that partition) will be updated. If primary keys do not match, new records will be added. Note that if dataset contain records with duplicate primary key values in the same partition, those duplicate records will be deduped into one updated record.
             
            * **DELETE** - Remove existing records in the dataset with primary key configured, events for datasets without primary keys are not allowed. If event data contains primary keys that match records in the dataset within same partition, then those existing records (in that partition) will be deleted. If primary keys do not match, no actions will be done. Note that if dataset contain records with duplicate primary key values in the same partition, all those duplicates will be removed.
            

            
          

          - **datasetLoadExecution** *(dict) --* 

            The target dataset load execution.

            
            

            - **status** *(string) --* 

              The event load execution status to target dataset.

              
            

            - **message** *(string) --* 

              The failure message (if any) of failed event load execution to dataset.

              
        
      
    
  
  **Exceptions**
  
  *   :py:class:`SupplyChain.Client.exceptions.ServiceQuotaExceededException`

  
  *   :py:class:`SupplyChain.Client.exceptions.ResourceNotFoundException`

  
  *   :py:class:`SupplyChain.Client.exceptions.ThrottlingException`

  
  *   :py:class:`SupplyChain.Client.exceptions.AccessDeniedException`

  
  *   :py:class:`SupplyChain.Client.exceptions.ValidationException`

  
  *   :py:class:`SupplyChain.Client.exceptions.InternalServerException`

  
  *   :py:class:`SupplyChain.Client.exceptions.ConflictException`

  